{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ee141b",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0322eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU, GELU, SELU, Mish\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a888a",
   "metadata": {},
   "source": [
    "## Selecting the device to run our code(GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21586bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb532212",
   "metadata": {},
   "source": [
    "### Giving the arg parse command option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ff5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using argparse, I have define the arguments and options that my program accepts,\n",
    "# and argparse will run the code, pass arguments from command line and \n",
    "# automatically generate help messages. I have given the defaults values for \n",
    "# all the arguments, so code can be run without passing any arguments.\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "# initializing the arguments to be passed\n",
    "parser = argparse.ArgumentParser(description=\"Stores all the hyperpamaters for the model.\")\n",
    "parser.add_argument(\"-wp\", \"--wandb_project\",default=\"cs6910_assignment2\" ,type=str,\n",
    "                    help=\"Enter the Name of your Wandb Project\")\n",
    "parser.add_argument(\"-we\", \"--wandb_entity\", default=\"am22s020\",type=str,\n",
    "                    help=\"Wandb Entity used to track experiments in the Weights & Biases dashboard.\")\n",
    "parser.add_argument(\"-ws\", \"--wandb_sweep\", default=\"False\", type=bool,\n",
    "                    help=\"If you want to run wandb sweep then give True\")\n",
    "parser.add_argument(\"-e\", \"--epochs\",default=\"1\", type=int, help=\"Number of epochs to train neural network.\")\n",
    "parser.add_argument(\"-b\", \"--batch_size\",default=\"16\", type=int, help=\"Batch size used to train neural network.\")\n",
    "parser.add_argument(\"-da\", \"--data_augmentation\", default=\"True\", type=bool, choices=[True, False])\n",
    "parser.add_argument(\"-opt\", \"--optimizer\", default=\"adam\", type=str, choices=[\"adam\", \"sgd\"])\n",
    "parser.add_argument(\"-opt\", \"--learning_rate\", default=\"0.0001\", type=int, choices=[0.001, 0.0001])\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "wandb_project = args.wandb_project\n",
    "wandb_entity = args.wandb_entity\n",
    "wandb_sweep = args.wandb_sweep\n",
    "num_epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "data_augmentation = args.data_augmentation\n",
    "optimizer = args.optimizer\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "print(\"wandb_project: \",wandb_project,\"wandb_entity: \",wandb_entity,\"wandb_sweep: \",wandb_sweep,\n",
    "      \"num_epochs :\", num_epochs , \"batch_size: \", batch_size, \"data_augmentation\",\n",
    "      data_augmentation,  \"optimizer: \", optimizer, \"learning rate: \",learning_rate) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ad595",
   "metadata": {},
   "source": [
    "## Uploading and transforming the dataset to train our CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a3247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_processing(batch_size, data_augmentation):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will upload the downloaded datasets(train and test).\n",
    "    Apply transformation on the dataset like resize to make each image of same size\n",
    "    as we know inaturalist datasets are having different sizes.\n",
    "    Split the train dataset into train(80%) and validation(20%).\n",
    "    Use transforms.compose method to reformat images for modeling,and save to variable\n",
    "    all_transforms for later use, find the size, mean and std for each channel of our dataset.\n",
    "    Again uploaded the train dataset and taken only 20% of it to make\n",
    "    augmented dataset.\n",
    "    I have applied horizontalFlip, Randomrotation to augment dataset with resize\n",
    "    and the normalization.\n",
    "    Then i concated the train and augmented datasets if data_augmentation=True.\n",
    "    I have created dataloader for train, validation and test datasets. dataloader \n",
    "    takes data in batches which saves our memory.\n",
    "    Then I have returned the dataloaders and datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                         transforms.ToTensor(),        #0-255 to 0-1 & numpy to tensor\n",
    "                                         transforms.Normalize(mean=[0.4713, 0.4600, 0.3897],  #0-1 to [-1,1]\n",
    "                                                              std=[0.2373, 0.2266, 0.2374])                                        \n",
    "                                         ])\n",
    "\n",
    "    # path for training and testing dataset directory\n",
    "    train_path = r\"C:\\Users\\HICLIPS-ASK\\nature_12K\\inaturalist_12K\\train\"\n",
    "    test_path = r\"C:\\Users\\HICLIPS-ASK\\nature_12K\\inaturalist_12K\\val\"\n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root = train_path, transform = all_transforms)\n",
    "    \n",
    "    # converting train dataset into train and validation for hyperparameter tuning\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # data augmentation\n",
    "    if data_augmentation == True:\n",
    "        augment_transforms = transforms.Compose([transforms.Resize((256,256)),                                  \n",
    "                                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                 transforms.RandomRotation((-60,60)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=[0.4713, 0.4600, 0.3897], \n",
    "                                                              std=[0.2373, 0.2266, 0.2374])\n",
    "                                                 \n",
    "                                         ])\n",
    "        \n",
    "        \n",
    "        # uploading train dataset to take a portion and augment it then concate with train dataset\n",
    "        aug_dataset = torchvision.datasets.ImageFolder(root = train_path, transform = augment_transforms)\n",
    "        discrad_size = int(0.8 * len(aug_dataset))\n",
    "        aug_size = len(aug_dataset) - discrad_size\n",
    "        \n",
    "        _ , transformed_dataset = torch.utils.data.random_split(aug_dataset, [discrad_size, aug_size])\n",
    "        train_dataset = torch.utils.data.ConcatDataset([transformed_dataset, train_dataset])\n",
    "\n",
    "    test_dataset = torchvision.datasets.ImageFolder(root = test_path, transform = all_transforms)\n",
    "\n",
    "    # Instantiate loader objects to facilitate processing\n",
    "    # shuffle= True, will ensure data of each class present in each batch\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader, train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59f3ef",
   "metadata": {},
   "source": [
    "## Fine-tuning the VGG16 CNN Model(Pretrained) to make compatible with our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_model(model):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will modify the pretrained imported model as VGG16, resnet, inception\n",
    "    as these huge network and fine tuning on even small dataset like iNaturalist\n",
    "    is very expensive. \n",
    "    VGG16 has 13 Convolutional layers and 3 linear dense layer, \n",
    "    i will freeze all the convolutional layers except the last two.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Replacing the last layer VGG16 to make it compatible with iNaturalist ten class dataset\n",
    "    model.classifier[6] = Linear(in_features=4096, out_features=10, bias=True)\n",
    "    # freezing the parameters of the convolutional layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # defreezing 13th conv layer of vgg16\n",
    "    for param in model.features[28].parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # defreezing 12th conv layer of vgg16\n",
    "    for param in model.features[26].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "        \n",
    "        \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1a9e7",
   "metadata": {},
   "source": [
    "## Function to find the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ef1c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    This function will calculate the accuracy on the given dataloader dataset.\n",
    "    It takes model and dataloader as arguments and return the accuracy.\n",
    "    First model is set into the .eval() mode to deactivate backpropagation.\n",
    "    Then images in the dataloader are send through forward function and \n",
    "    compared with the labels to match max value in output with the label class.\n",
    "    Correct term initialized to collect the number of correct prediction and total\n",
    "    is to count the total number of images.\n",
    "    finally we get the accuracy by, accuracy = 100 * correct / total formula.\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2a881",
   "metadata": {},
   "source": [
    "## Creating training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d0b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un comment if you are running the code manually i.e not by command line arguments\n",
    "wandb_project, wandb_entity, wandb_sweep, optimizer = \"cs6910_assignment2_partA_new\",\"am22s020\",False,'adam'\n",
    "num_epochs, batch_size, data_augmentation, learning_rate = 5, 16, False, 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3469a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(optimizer,batch_size, data_augmentation,device,\n",
    "              num_epochs,learning_rate):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is created to bring all the inputs needed to train the modified VGG16\n",
    "    CNN model and calculate the train loss, train accuracy and validation accuracy.\n",
    "    First we have choosen Adam as optimizer with learning rate= 1e-4, and weight\n",
    "    decay = 1e-4. Then cross entropy loss is choosen as the loss function.\n",
    "    data_pre_processing function initialized to give the dataloders and the datasets\n",
    "    needed to train and evaluate the model.\n",
    "    model is set to .train() mode to do backprop with forward prop to train the model.\n",
    "    In training the images in batch are taken and feed to forward prop to get loss \n",
    "    value then backprop is performed then the weights and biases are updated.\n",
    "    After training for each epoch train accuracy and validation accuracy are calculated.\n",
    "    Best model is saved for further use like to calculate test accuracy.\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    print(\"Training the model in progress==>>\")\n",
    "    if wandb_sweep == True:\n",
    "        #default values for wandb run\n",
    "        config_defaults = {\n",
    "            'optimizer': 'adam',\n",
    "            'data_augmentation': True,\n",
    "            'num_epochs': 5,\n",
    "            'learning_rate': 0.0001\n",
    "             }\n",
    "\n",
    "        #initialize wandb\n",
    "        wandb.init(project = wandb_project,config=config_defaults)\n",
    "\n",
    "        # config is a data structure that holds hyperparameters and inputs\n",
    "        config = wandb.config\n",
    "\n",
    "        # Local variables, values obtained from wandb config\n",
    "        data_augmentation = config.data_augmentation\n",
    "        optimizer = config.optimizer\n",
    "        num_epochs = config.num_epochs\n",
    "        learning_rate = config.learning_rate\n",
    "\n",
    "        # Defining the run name in wandb sweep\n",
    "        wandb.run.name  = \"lr_{}_DA_{}_opt_{}_epoch_{}_\".format(learning_rate,\n",
    "                                                                data_augmentation,\n",
    "                                                                  optimizer,                                                                         \n",
    "                                                                  num_epochs)                                                                                                                                                 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(wandb.run.name )\n",
    "    \n",
    "    # uploading the vgg16 pretrained model\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "    # modify the VGG16 model \n",
    "    model = modified_model(model)\n",
    "    # sending model to device(GPU or CPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #Optimizer and loss function\n",
    "    if optimizer == 'adam':\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay=0.0001)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "        \n",
    "    loss_function=nn.CrossEntropyLoss()\n",
    "    \n",
    "    # uploading the dataloader and datasets\n",
    "    train_loader, test_loader, val_loader, train_dataset,test_dataset = data_pre_processing(batch_size, data_augmentation)\n",
    "    \n",
    "    \n",
    "    # Training on training dataset\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)               \n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                train_loss=running_loss/100   \n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Evaluate training set accuracy\n",
    "        train_accuracy = evaluate(model, train_loader)\n",
    "\n",
    "        # Evaluate test set accuracy\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "        # printing the epoch, train loss, train accuracy and validation accuracy after each epoch\n",
    "        print(\"Epoch: \"+str(epoch+1)+ ' Train Loss: '+ str(train_loss) +' Train Accuracy: '+\n",
    "              str(train_accuracy) + ' Validation Accuracy: '+ str(val_accuracy)) \n",
    "        \n",
    "        if wandb_sweep == True:\n",
    "            wandb.log({\"validation accuracy\": val_accuracy, \"train accuracy\": train_accuracy, \n",
    "                        \"train loss\": train_loss, 'epoch': epoch+1})\n",
    "\n",
    "    if wandb_sweep == True:\n",
    "        wandb.run.name \n",
    "        wandb.save()\n",
    "        wandb.run.finish()\n",
    "        \n",
    "    # if code running on local machine returning the model    \n",
    "    if wandb_sweep == False:\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d641bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_sweep == False:\n",
    "    train_CNN(optimizer,batch_size, data_augmentation,device, \n",
    "              num_epochs,learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_sweep == True:\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe40a88",
   "metadata": {},
   "source": [
    "### Wandb Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8491cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep():\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is used to exploit the wandb hyperparameter sweep \n",
    "    function to get the best hyperparameters.\n",
    "    It takes in no inputs and gives no outputs.\n",
    "    Instead it logs everything into the wandb workspace'''\n",
    "\n",
    "    \"\"\"\n",
    "    sweep_config = {\"name\": wandb_project, \"method\": \"bayes\"}   \n",
    "    sweep_config[\"metric\"] = {\"name\": \"val_accuracy\", \"goal\": \"maximize\"}\n",
    "\n",
    "    #Declaring the dictionary of all choices for the hyperparameters.\n",
    "    parameters_dict = {\n",
    "                  'optimizer': {\"values\": ['adam', 'sgd']}, \n",
    "                  \"data_augmentation\": {\"values\": [True, False]},\n",
    "                  \"num_epochs\": {\"values\": [5, 10]},\n",
    "                 \"learning_rate\": {\"values\": [0.0001, 0.001]}\n",
    "                    }\n",
    "    sweep_config[\"parameters\"] = parameters_dict\n",
    "    \n",
    "    # creating the sweep id and starting the sweep agent to run the hyper parameter configuration\n",
    "    sweep_id = wandb.sweep(sweep_config, entity=wandb_entity, project=wandb_project)\n",
    "    wandb.agent(sweep_id, train_CNN(optimizer,batch_size, data_augmentation,device, \n",
    "              num_epochs,learning_rate) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54502171",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_sweep == True:\n",
    "    sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823b27d",
   "metadata": {},
   "source": [
    "### Test the model on the best configuration of hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0952c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(optimizer, learning_rate,batch_size,\n",
    "                               num_epochs, data_augmentation):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function test the model on the best parameters find out after \n",
    "    hyper parameter tuning using wandb.\n",
    "    set the arguments of the best validation accuracy run in the ConvNeuNet().\n",
    "    This function train the model on the best parameters and give the test accuracy \n",
    "    on the test dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Testing the model===>>>\")\n",
    "    \n",
    "     # uploading the vgg16 pretrained model\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "    # modify the VGG16 model \n",
    "    model = modified_model(model)\n",
    "    # sending model to device(GPU or CPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #Optimizer and loss function\n",
    "    if optimizer == 'adam':\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.0001)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer=torch.optim.SGD(model.parameters(),lr=0.0001)\n",
    "        \n",
    "    loss_function=nn.CrossEntropyLoss()\n",
    "    \n",
    "    # uploading the dataloader and datasets\n",
    "    train_loader, test_loader, val_loader, train_dataset,test_dataset = data_pre_processing(batch_size, data_augmentation)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "     # Training on training dataset\n",
    "    best_accuracy = 0.0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            if epoch == 9:\n",
    "                print(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if epoch == 9:\n",
    "                print(outputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                train_loss=running_loss/100   \n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Evaluate training set accuracy\n",
    "        train_accuracy = evaluate(model, train_loader)\n",
    "\n",
    "        # Evaluate test set accuracy\n",
    "        test_accuracy = evaluate(model, test_loader)\n",
    "        \n",
    "        #Save the best model\n",
    "        if test_accuracy>best_accuracy:\n",
    "            torch.save(model.state_dict(), 'best_partB_checkpoint.model')\n",
    "            best_accuracy=test_accuracy\n",
    "\n",
    "\n",
    "        print(\"Epoch: \"+str(epoch+1)+ ' Train Loss:'+ str(train_loss) +' Train Accuracy:'+\n",
    "              str(train_accuracy) + ' Test Accuracy: '+ str(test_accuracy))\n",
    "        \n",
    "    return model\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b3b64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model===>>>\n",
      "Epoch: 1 Train Loss:0.7904532153904438 Train Accuracy:87.18589823727966 Test Accuracy: 78.25\n",
      "Epoch: 2 Train Loss:0.4261125203594565 Train Accuracy:96.82460307538442 Test Accuracy: 79.35\n",
      "Epoch: 3 Train Loss:0.1398056647274643 Train Accuracy:98.29978747343418 Test Accuracy: 77.65\n",
      "Epoch: 4 Train Loss:0.12736140274675564 Train Accuracy:98.54981872734092 Test Accuracy: 76.9\n",
      "Epoch: 5 Train Loss:0.10142865586094559 Train Accuracy:98.01225153144144 Test Accuracy: 76.15\n"
     ]
    }
   ],
   "source": [
    "model = test_model(optimizer='adam', learning_rate=0.0001,batch_size=16,\n",
    "                               num_epochs=5, data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590fb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
