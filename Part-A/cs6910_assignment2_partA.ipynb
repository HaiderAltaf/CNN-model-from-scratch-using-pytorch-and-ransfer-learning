{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c091e0",
   "metadata": {},
   "source": [
    "# Part-A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2830da",
   "metadata": {},
   "source": [
    "### lmport  the relevant libraries and inbuilt functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU, GELU, SELU, Mish\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4d71c",
   "metadata": {},
   "source": [
    "### Selecting the device to run our code(GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0079d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66034bbc",
   "metadata": {},
   "source": [
    "### Giving the arg parse command option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a6bbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-wp WANDB_PROJECT] [-we WANDB_ENTITY] [-ws WANDB_SWEEP] [-e EPOCHS] [-b BATCH_SIZE]\n",
      "                             [-a {selu,gelu,relu}] [-dl {50,100,150,200}] [-da DATA_AUGMENTATION]\n",
      "                             [-bn BATCH_NORMALISATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\itsal\\AppData\\Roaming\\jupyter\\runtime\\kernel-efef4b7f-49c3-4a86-8941-18122239e9eb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsal\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using argparse, I have define the arguments and options that my program accepts,\n",
    "# and argparse will run the code, pass arguments from command line and \n",
    "# automatically generate help messages. I have given the defaults values for \n",
    "# all the arguments, so code can be run without passing any arguments.\n",
    "# lastly returning the arguments to be used in the running of the code.\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Stores all the hyperpamaters for the model.\")\n",
    "parser.add_argument(\"-wp\", \"--wandb_project\",default=\"cs6910_assignment2_new\" ,type=str,\n",
    "                    help=\"Enter the Name of your Wandb Project\")\n",
    "parser.add_argument(\"-we\", \"--wandb_entity\", default=\"am22s020\",type=str,\n",
    "                    help=\"Wandb Entity used to track experiments in the Weights & Biases dashboard.\")\n",
    "parser.add_argument(\"-ws\", \"--wandb_sweep\", default=\"False\", type=bool,\n",
    "                    help=\"If you want to run wandb sweep then give True\")\n",
    "parser.add_argument(\"-e\", \"--epochs\",default=\"1\", type=int, help=\"Number of epochs to train neural network.\")\n",
    "parser.add_argument(\"-b\", \"--batch_size\",default=\"4\", type=int, help=\"Batch size used to train neural network.\")\n",
    "parser.add_argument(\"-a\", \"--activation\",default=\"selu\", type=str, choices=[\"selu\", \"gelu\", \"relu\"])\n",
    "parser.add_argument(\"-dl\", \"--dense_layer\",default=\"100\", type=int, \n",
    "                    choices=[50,100,150,200], help=\"Choose number of neuron in dense layer\")\n",
    "parser.add_argument(\"-da\", \"--data_augmentation\", default=\"True\", type=bool)\n",
    "parser.add_argument(\"-bn\", \"--batch_normalisation\", default=\"True\", type=bool)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "wandb_project = args.wandb_project\n",
    "wandb_entity = args.wandb_entity\n",
    "wandb_sweep = args.wandb_sweep\n",
    "num_epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "act_fu = args.activation\n",
    "size_denseLayer = args.dense_layer\n",
    "data_augmentation = args.data_augmentation\n",
    "batch_normalisation = args.batch_normalisation\n",
    "\n",
    "print(\"wandb_project :\", wandb_project , \"wandb_entity: \", wandb_entity,\"wandb_sweep: \",wandb_sweep,\n",
    "      \"epochs: \",num_epochs,\"batch_size: \",batch_size, \"dense_layer: \", size_denseLayer,\n",
    "      \"activation_function: \", act_fu,\"data augmentation: \", data_augmentation, \n",
    "      \"batch Normalization: \", batch_normalisation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560c4a1",
   "metadata": {},
   "source": [
    "###  Uploading and transforming the dataset to train our CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2154975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transforms.compose method to reformat images for modeling,\n",
    "# and save to variable all_transforms for later use, find the size, mean and std for each channel of our dataset\n",
    "def data_pre_processing(batch_size, data_augmentation):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will upload the downloaded datasets(train and test).\n",
    "    Apply transformation on the dataset like resize to make each image of same size\n",
    "    as we know inaturalist datasets are having different sizes.\n",
    "    Split the train dataset into train(80%) and validation(20%).\n",
    "    Use transforms.compose method to reformat images for modeling,and save to variable\n",
    "    all_transforms for later use, find the size, mean and std for each channel of our dataset.\n",
    "    Again uploaded the train dataset and taken only 20% of it to make\n",
    "    augmented dataset.\n",
    "    I have applied horizontalFlip, Randomrotation to augment dataset with resize\n",
    "    and the normalization.\n",
    "    Then i concated the train and augmented datasets if data_augmentation=True.\n",
    "    I have created dataloader for train, validation and test datasets. dataloader \n",
    "    takes data in batches which saves our memory.\n",
    "    Then I have returned the dataloaders and datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Transformation on the dataset. Resize=> numpy to tensor => Normalizing the pixel values\n",
    "    all_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                         transforms.ToTensor(),        #0-255 to 0-1 & numpy to tensor\n",
    "                                         transforms.Normalize(mean=[0.4713, 0.4600, 0.3897],  #0-1 to [-1,1]\n",
    "                                                              std=[0.2373, 0.2266, 0.2374])                                        \n",
    "                                         ])\n",
    "\n",
    "    # path for training and testing dataset directory\n",
    "    train_path = r\"C:\\Users\\HICLIPS-ASK\\nature_12K\\inaturalist_12K\\train\"\n",
    "    test_path = r\"C:\\Users\\HICLIPS-ASK\\nature_12K\\inaturalist_12K\\val\"\n",
    "    \n",
    "    # uploading the train data with above transformation applied\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root = train_path, transform = all_transforms)\n",
    "    \n",
    "    # converting train dataset into train and validation for hyperparameter tuning\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    \n",
    "    # splitting the train data inti train and validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # data augmentation\n",
    "    if data_augmentation == True:\n",
    "        # These transformation will be applied on the augmented dataset \n",
    "        augment_transforms = transforms.Compose([transforms.Resize((256,256)),                                         \n",
    "                                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                 transforms.RandomRotation((-60,60)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=[0.4713, 0.4600, 0.3897], \n",
    "                                                              std=[0.2373, 0.2266, 0.2374])\n",
    "                                                 \n",
    "                                         ])\n",
    "        \n",
    "        \n",
    "        # uploading train dataset to take a portion as augment dataset, then concate with train dataset\n",
    "        aug_dataset = torchvision.datasets.ImageFolder(root = train_path, transform = augment_transforms)\n",
    "        discrad_size = int(0.8 * len(aug_dataset))\n",
    "        aug_size = len(aug_dataset) - discrad_size\n",
    "        \n",
    "        _ , transformed_dataset = torch.utils.data.random_split(aug_dataset, [discrad_size, aug_size])\n",
    "        train_dataset = torch.utils.data.ConcatDataset([transformed_dataset, train_dataset])\n",
    "    \n",
    "    # uploading the test dataset\n",
    "    test_dataset = torchvision.datasets.ImageFolder(root = test_path, transform = all_transforms)\n",
    "\n",
    "    # Instantiate loader objects to facilitate processing\n",
    "    # shuffle= True, will ensure data of each class present in each batch\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader, train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775f30f",
   "metadata": {},
   "source": [
    "### Creating CNN Model from scratch using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f160000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuNet(Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    In __init__, I have initialized the layers to be used in the model.\n",
    "    My model has five consecutive convolutional layers, each layer has set of\n",
    "    2D convolution => Batch Nromalisation => non-linear activation function => Dropout => max pool layers.\n",
    "    The code is flexible such that the number of filters, size of filters, and activation function of the \n",
    "    convolution layers and dense layers can be changed. We can change the number of neurons in the dense layer.\n",
    "    I have created a class named ConvNeuNet(nn.Modeule), in this nitialised the init function with arguments\n",
    "    having flexible inputs. In init initialised all the convolution layers, dense layer and output layer.\n",
    "    Within the class, I have created forward() function in which all the layers are arranged in sequence.\n",
    "    The output of forward is a tensor having probability of ten classes.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size_kernel, num_stride, act_fu, size_denseLayer,\n",
    "                 data_augmentation, batch_normalisation,padding, dropout_rate,\n",
    "                 num_filters,classes=10,input_channels=3):\n",
    "        \n",
    "        # call the parent constructor\n",
    "        super(ConvNeuNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm = batch_normalisation\n",
    "        self.data_aug = data_augmentation\n",
    "        width = 256\n",
    "        height = 256\n",
    "        \n",
    "        #(batch_size = 64, input_channels=3, width=150, height=150)\n",
    "         # initialize second set of CONV => update dim => Batch Nrom => RELU => Dropout => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=input_channels, out_channels=num_filters[0],\n",
    "                    kernel_size=size_kernel[0], stride=num_stride, padding=padding)\n",
    "        width = int((width- size_kernel[0][0] + 2*padding)/num_stride) + 1\n",
    "        height = int((height- size_kernel[0][0] + 2*padding)/num_stride) + 1\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=num_filters[0])\n",
    "        self.af1 = ReLU() if act_fu=='relu' else GELU() if act_fu=='gelu' else SELU() if act_fu=='selu' else Mish()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        width = int((width- 2 + 2*0)/2) + 1\n",
    "        height = int((height- 2 + 2*0)/2) + 1\n",
    "        # initialize second set of CONV => update dim => Batch Nrom => RELU => Dropout => POOL layers\n",
    "        \n",
    "        self.conv2 = Conv2d(in_channels=num_filters[0], out_channels=num_filters[1],\n",
    "                     kernel_size=size_kernel[1], stride=num_stride, padding=padding)\n",
    "        width = int((width- size_kernel[1][0] + 2*padding)/num_stride) + 1\n",
    "        height = int((height- size_kernel[1][0] + 2*padding)/num_stride) + 1\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=num_filters[1])\n",
    "        self.af2 = ReLU() if act_fu=='relu' else GELU() if act_fu=='gelu' else SELU() if act_fu=='selu' else Mish()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        width = int((width- 2 + 2*0)/2) + 1\n",
    "        height = int((height- 2 + 2*0)/2) + 1\n",
    "        \n",
    "         # initialize second set of CONV => update dim => Batch Nrom => RELU => Dropout => POOL layers\n",
    "        self.conv3 = Conv2d(in_channels=num_filters[1], out_channels=num_filters[2],\n",
    "                     kernel_size=size_kernel[2], stride=num_stride, padding=padding)\n",
    "        width = int((width- size_kernel[2][0] + 2*padding)/num_stride) + 1\n",
    "        height = int((height- size_kernel[2][0] + 2*padding)/num_stride) + 1\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=num_filters[2])\n",
    "        self.af3 = ReLU() if act_fu=='relu' else GELU() if act_fu=='gelu' else SELU() if act_fu=='selu' else Mish()\n",
    "        self.dropout3 = nn.Dropout(p=dropout_rate)\n",
    "        self.maxpool3 = MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "         # updating width and height of the next layer after maxpool\n",
    "        width = int((width- 2 + 2*0)/2) + 1\n",
    "        height = int((height- 2 + 2*0)/2) + 1\n",
    "        \n",
    "         # initialize second set of CONV => update dim => Batch Nrom => RELU => Dropout => POOL layers\n",
    "        self.conv4 = Conv2d(in_channels=num_filters[2], out_channels=num_filters[3],\n",
    "                     kernel_size=size_kernel[3], stride=num_stride, padding=padding)\n",
    "        width = int((width- size_kernel[3][0] + 2*padding)/num_stride) + 1\n",
    "        height = int((height- size_kernel[3][0] + 2*padding)/num_stride) + 1\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=num_filters[3])\n",
    "        self.af4 = ReLU() if act_fu=='relu' else GELU() if act_fu=='gelu' else SELU() if act_fu=='selu' else Mish()\n",
    "        self.dropout4 = nn.Dropout(p=dropout_rate)\n",
    "        self.maxpool4 = MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        width = int((width- 2 + 2*0)/2) + 1\n",
    "        height = int((height- 2 + 2*0)/2) + 1\n",
    "        \n",
    "         # initialize second set of CONV => update dim => Batch Nrom => RELU => Dropout => POOL layers\n",
    "        self.conv5 = Conv2d(in_channels=num_filters[3], out_channels=num_filters[4],\n",
    "                     kernel_size=size_kernel[4], stride=num_stride, padding=padding)\n",
    "        width = int((width- size_kernel[4][0] + 2*padding)/num_stride) + 1\n",
    "        height = int((height- size_kernel[4][0] + 2*padding)/num_stride) + 1\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=num_filters[4])\n",
    "        self.af5 = ReLU() if act_fu=='relu' else GELU() if act_fu=='gelu' else SELU() if act_fu=='selu' else Mish()\n",
    "        self.dropout5 = nn.Dropout(p=dropout_rate)\n",
    "        self.maxpool5 = MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        width = int((width- 2 + 2*0)/2) + 1\n",
    "        height = int((height- 2 + 2*0)/2) + 1\n",
    "        #initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=int(num_filters[4]*width*height), out_features=size_denseLayer)\n",
    "        self.af6 = ReLU() if act_fu=='relu' else GELU() if act_fu=='gelu' else SELU() if act_fu=='selu' else Mish()\n",
    "        self.dropout6 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=size_denseLayer, out_features=classes)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pass the input through our first set of CONV => Batch_norm => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.af1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.maxpool1(x)\n",
    "       \n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => Batch_norm => RELU => layers\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.af2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # pass the output from the previous layer through the third\n",
    "        # set of CONV => Batch_norm => RELU => POOL layers\n",
    "        x = self.conv3(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn3(x)\n",
    "        x = self.af3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        # pass the output from the previous layer through the fourth\n",
    "        # set of CONV => Batch_norm => RELU => POOL layers\n",
    "        x = self.conv4(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn4(x)\n",
    "        x = self.af4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        # pass the output from the previous layer through the fifth\n",
    "        # set of CONV => Batch_norm => RELU => POOL layers\n",
    "        x = self.conv5(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn5(x)\n",
    "        x = self.af5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.af6(x)\n",
    "        x = self.dropout6(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1979492",
   "metadata": {},
   "source": [
    "### Define the function to find the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    This function will calculate the accuracy on the given dataloader dataset.\n",
    "    It takes model and dataloader as arguments and return the accuracy.\n",
    "    First model is set into the .eval() mode to deactivate backpropagation.\n",
    "    Then images in the dataloader are send through forward function and \n",
    "    compared with the labels to match max value in output with the label class.\n",
    "    Correct term initialized to collect the number of correct prediction and total\n",
    "    is to count the total number of images.\n",
    "    finally we get the accuracy by, accuracy = 100 * correct / total formula.\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799536eb",
   "metadata": {},
   "source": [
    "### Creating function to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to run the code manually un-comment the below line \n",
    "# wandb_sweep, wandb_project, num_epochs, batch_size, data_augmentation= True,\"cs6910_assignment2_partA_new\", 1, 16,True\n",
    "# wandb_entity, act_fu,size_denseLayer,batch_normalisation = \"am22s020\",'selu', 100, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and import the wandb \n",
    "if wandb_sweep == True:\n",
    "    #!pip install wandb\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb10e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't un comment it\n",
    "num_filters, size_kernel,num_stride,padding,dropout_rate = [12,12,12,12,12],[(3,3),(3,3),(3,3),(3,3),(3,3)],1,1,0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7700f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_CNN(num_filters, size_kernel,num_stride,padding,dropout_rate,\n",
    "             wandb_sweep, wandb_project, num_epochs, batch_size,data_augmentation,\n",
    "              wandb_entity, act_fu,size_denseLayer,batch_normalisation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Created train_CNN() function to train our model.\n",
    "    In this, first model is defined and it is exported to the device (either GPU or CPU).\n",
    "    Optimizer and loss function is imported using torch library. \n",
    "    we have choosen Adam as optimizer with learning rate= 1e-4, and weight\n",
    "    decay = 1e-4. Then cross entropy loss is choosen as the loss function.\n",
    "    I have also include the commands needed to integrate the wandb sweep. \n",
    "    If wandb_sweep == True, then sweep will start otherwise training will be shown only in our environment.\n",
    "    I have login to wandb account. I have already imported the wandb,\n",
    "    now I am giving the default values of our variable for sweep. \n",
    "    After that I have defined the wandb run name which will be assign to each run.\n",
    "    Values like epoch, train loss, train accuracy and validation accuracy are login to wandb.\n",
    "    Saving the wandb run and finishing the run.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if wandb_sweep == True:\n",
    "        #default values for wandb run\n",
    "        config_defaults = {\n",
    "            'num_filters': [12,12,12,12,12],\n",
    "            'act_fu': 'relu',\n",
    "            'size_kernel': [(3,3),(3,3),(3,3),(3,3),(3,3)],\n",
    "            'data_augmentation': True,\n",
    "            'batch_normalisation': True,\n",
    "            'dropout_rate': 0.2,\n",
    "            'size_denseLayer':200\n",
    "             }\n",
    "    \n",
    "\n",
    "        #initialize wandb\n",
    "        wandb.init(project = wandb_project ,config=config_defaults)\n",
    "\n",
    "        # config is a data structure that holds hyperparameters and inputs\n",
    "        config = wandb.config\n",
    "\n",
    "        # Local variables, values obtained from wandb config\n",
    "        num_filters = config.num_filters\n",
    "        act_fu = config.act_fu\n",
    "        size_kernel = config.size_kernel\n",
    "        data_augmentation = config.data_augmentation\n",
    "        batch_normalisation = config.batch_normalisation\n",
    "        dropout_rate = config.dropout_rate\n",
    "        size_denseLayer = config.size_denseLayer\n",
    "\n",
    "        # Defining the run name in wandb sweep\n",
    "        wandb.run.name  = \"FSize_{}_af_{}_NF_{}_DA_{}_BN_{}_Drp_{}_DLayer_{}_\".format(size_kernel,\n",
    "                                                                              act_fu,\n",
    "                                                                              num_filters,\n",
    "                                                                              data_augmentation,\n",
    "                                                                              batch_normalisation,\n",
    "                                                                              dropout_rate,\n",
    "                                                                              size_denseLayer)\n",
    "\n",
    "\n",
    "        print(wandb.run.name )\n",
    "    \n",
    "\n",
    "    #loading the dataloaders and dataset\n",
    "    train_loader, test_loader, val_loader, train_dataset, test_dataset = data_pre_processing(batch_size,\n",
    "                                                                                        data_augmentation=True)\n",
    "    # Loading the cnn model    \n",
    "    model = ConvNeuNet(size_kernel, num_stride, act_fu, size_denseLayer,\n",
    "                 data_augmentation, batch_normalisation,padding, dropout_rate,\n",
    "                 num_filters,classes=10, input_channels=3).to(device)\n",
    "    \n",
    "    #Optimizer and loss function\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.0001)\n",
    "    loss_function=nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "       \n",
    "    # Training on training dataset\n",
    "    # setting model to train mode\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # loading the images and labels to the device\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # for each train loader initializing the gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            # getting output from the model\n",
    "            outputs = model(inputs)\n",
    "            # calculating the loss for each batch\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # doing backprop\n",
    "            loss.backward()\n",
    "            # Updating the parameters\n",
    "            optimizer.step()\n",
    "            # adding the train loss of each batch\n",
    "            running_loss += loss.item()\n",
    "            # storing the loss of last 100 batches\n",
    "            if i % 100 == 99:\n",
    "                train_loss=running_loss/100   \n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Evaluate training set accuracy\n",
    "        train_accuracy = evaluate(model, train_loader)\n",
    "\n",
    "        # Evaluate test set accuracy\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "\n",
    "        print(\"Epoch: \"+str(epoch+1)+ ' Train Loss:'+ str(train_loss) +' Train Accuracy:'+\n",
    "              str(train_accuracy) + ' Validation Accuracy: '+ str(val_accuracy)) \n",
    "\n",
    "\n",
    "        if wandb_sweep == True:\n",
    "        \n",
    "            wandb.log({\"validation accuracy\": val_accuracy, \"train accuracy\": train_accuracy, \n",
    "                        \"train loss\": train_loss, 'epoch': epoch+1})\n",
    "            \n",
    "    if wandb_sweep == True:\n",
    "        wandb.run.name \n",
    "        wandb.run.save()\n",
    "        wandb.run.finish()\n",
    "        \n",
    "    if wandb_sweep == False:\n",
    "        return model\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f342eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_sweep == False:\n",
    "    model = train_CNN(num_filters, size_kernel,num_stride,padding,dropout_rate,\n",
    "             wandb_sweep, wandb_project, num_epochs, batch_size,data_augmentation,\n",
    "              wandb_entity, act_fu,size_denseLayer,batch_normalisation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e241fd8",
   "metadata": {},
   "source": [
    "# Running the wandb sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep():\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is used to exploit the wandb hyperparameter sweep \n",
    "    function to get the best hyperparameters.\n",
    "\n",
    "    It takes in no inputs and gives no outputs.\n",
    "  \n",
    "    Instead it logs everything into the wandb workspace'''\n",
    "\n",
    "    \"\"\"\n",
    "    sweep_config = {\"name\": wandb_project, \"method\": \"bayes\"}   \n",
    "    sweep_config[\"metric\"] = {\"name\": \"val_accuracy\", \"goal\": \"maximize\"}\n",
    "\n",
    "    parameters_dict = {\n",
    "                  \"num_filters\": {\"values\": [[12,12,12,12,12],[4,8,16,32,64],[64,32,16,8,4]]},\n",
    "                  \"act_fu\": {\"values\": [\"relu\",\"selu\",\"gelu\",\"mish\"]},\n",
    "                  \"size_kernel\": {\"values\": [[(3,3),(3,3),(3,3),(3,3),(3,3)], [(3,3),(5,5),(5,5),(7,7),(7,7)],\n",
    "                                             [(7,7),(7,7),(5,5),(5,5),(3,3)]]}, \n",
    "                    \"data_augmentation\": {\"values\": [True, False]} ,\n",
    "                    \"batch_normalisation\": {\"values\": [True, False]} ,\n",
    "                    \"dropout_rate\": {\"values\": [0, 0.2, 0.3]},\n",
    "                    \"size_denseLayer\": {\"values\": [50, 100, 150, 200]}\n",
    "                    }\n",
    "    sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, entity=wandb_entity, project=wandb_project)\n",
    "    wandb.agent(sweep_id, train_CNN(num_filters, size_kernel,num_stride,padding,dropout_rate,\n",
    "             wandb_sweep, wandb_project, num_epochs, batch_size,data_augmentation,\n",
    "              wandb_entity, act_fu,size_denseLayer,batch_normalisation), count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266976ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_sweep == True:\n",
    "    sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b24bf",
   "metadata": {},
   "source": [
    "### testing the model on best sweep parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ad137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    \n",
    "    \"\"\"\n",
    "    This function test the model on the best parameters find out after \n",
    "    hyper parameter tuning using wandb.\n",
    "    set the arguments of the best validation accuracy run in the ConvNeuNet().\n",
    "    This function train the model on the best parameters and give the test accuracy \n",
    "    on the test dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Testing the model on the best parameters===>>>\")\n",
    "    \n",
    "    model = ConvNeuNet(size_kernel=[(3,3),(3,3),(3,3),(3,3),(3,3)], num_stride=1, act_fu='selu', size_denseLayer=200,\n",
    "                     data_augmentation=True, batch_normalisation=True, input_channels=3,\n",
    "                     classes=10, padding=1, dropout_rate=0.3, num_filters=[12,12,12,12,12]).to(device)\n",
    "\n",
    "    #Optimizer and loss function\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.0001)\n",
    "    loss_function=nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs=5\n",
    "\n",
    "    train_loader, test_loader, val_loader, train_dataset, test_dataset = data_pre_processing(batch_size=16,\n",
    "                                                                                        data_augmentation=True)\n",
    "    \n",
    "    \n",
    "\n",
    "     # Training on training dataset\n",
    "    best_accuracy = 0.0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            if epoch == 9:\n",
    "                print(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if epoch == 9:\n",
    "                print(outputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                train_loss=running_loss/100   \n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Evaluate training set accuracy\n",
    "        train_accuracy = evaluate(model, train_loader)\n",
    "\n",
    "        # Evaluate test set accuracy\n",
    "        test_accuracy = evaluate(model, test_loader)\n",
    "        \n",
    "        #Save the best model\n",
    "        if test_accuracy>best_accuracy:\n",
    "            torch.save(model.state_dict(), 'best_partA_checkpoint.model')\n",
    "            best_accuracy=test_accuracy\n",
    "\n",
    "\n",
    "        print(\"Epoch: \"+str(epoch+1)+ ' Train Loss:'+ str(train_loss) +' Train Accuracy:'+\n",
    "              str(train_accuracy) + ' Test Accuracy: '+ str(test_accuracy))\n",
    "        \n",
    "    return model\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e571c",
   "metadata": {},
   "source": [
    "### testing the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23065b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
