{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c091e0",
   "metadata": {},
   "source": [
    "# Part-A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b913d",
   "metadata": {},
   "source": [
    "## Question-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9b45d",
   "metadata": {},
   "source": [
    "Source1- https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d16aba",
   "metadata": {},
   "source": [
    "source2- https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2830da",
   "metadata": {},
   "source": [
    "### load Relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7130a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb87051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0079d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52169988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1b26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a3f9d60",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2154975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transforms.compose method to reformat images for modeling,\n",
    "# and save to variable all_transforms for later use, find the size, mean and std for each channel of our dataset\n",
    "def data_pre_processing(batch_size=64, data_augmentation=False):\n",
    "    \n",
    "    all_transforms = transforms.Compose([transforms.Resize((150,150)),\n",
    "                                         transforms.ToTensor(),        #0-255 to 0-1 & numpy to tensor\n",
    "                                         transforms.Normalize(mean=[0.4713, 0.4600, 0.3897],  #0-1 to [-1,1]\n",
    "                                                              std=[0.2373, 0.2266, 0.2374])\n",
    "                                         ])\n",
    "\n",
    "    # path for training and testing dataset directory\n",
    "    train_path = r\"C:\\Users\\HICLIPS-ASK\\nature_12K\\inaturalist_12K\\train\"\n",
    "    test_path = r\"C:\\Users\\HICLIPS-ASK\\nature_12K\\inaturalist_12K\\val\"\n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root = train_path, transform = all_transforms)\n",
    "    \n",
    "    # converting train dataset into train and validation for hyperparameter tuning\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # data augmentation\n",
    "    if data_augmentation == True:\n",
    "        augment_transforms = transforms.Compose([transforms.Resize((150,150)),                                         \n",
    "                                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                 transforms.RandomRotation((-60,60)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=[0.4713, 0.4600, 0.3897], \n",
    "                                                              std=[0.2373, 0.2266, 0.2374]),\n",
    "                                                 \n",
    "                                         ])\n",
    "        \n",
    "        \n",
    "        # uploading train dataset to take a portion and augment it then concate with train dataset\n",
    "        aug_dataset = torchvision.datasets.ImageFolder(root = train_path, transform = augment_transforms)\n",
    "        discrad_size = int(0.8 * len(aug_dataset))\n",
    "        aug_size = len(aug_dataset) - discrad_size\n",
    "        \n",
    "        _ , transformed_dataset = torch.utils.data.random_split(aug_dataset, [discrad_size, aug_size])\n",
    "        train_dataset = torch.utils.data.ConcatDataset([transformed_dataset, train_dataset])\n",
    "\n",
    "    test_dataset = torchvision.datasets.ImageFolder(root = test_path, transform = all_transforms)\n",
    "\n",
    "    # Instantiate loader objects to facilitate processing\n",
    "    # shuffle= True, will ensure data of each class present in each batch\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                               batch_size = batch_size,\n",
    "                                               shuffle = True)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader, train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2bf297",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, val_loader, train_dataset, test_dataset = data_pre_processing(batch_size=64,\n",
    "                                                                                        data_augmentation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c44c9",
   "metadata": {},
   "source": [
    "### Finding the mean and std of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0822e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataloader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe9488b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_mean_and_std(train_loader)\n",
    "#[0.4713, 0.4600, 0.3897], [0.2373, 0.2266, 0.2374] for iNaturalist dataset at resize=[150,150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0941b20",
   "metadata": {},
   "source": [
    "### Show image of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1bba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = True)\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch\n",
    "    print(images.shape)\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow = 3)\n",
    "    plt.figure(figsize= (11,11))\n",
    "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "    print('labels', labels)                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c29890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288c73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addition():\n",
    "    def __init__(self, x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def add(self):\n",
    "        return self.x +self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a058547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addition(0,4).add()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807eb13e",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f160000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuNet(Module):\n",
    "    \n",
    "    def __init__(self, size_kernel=3, num_stride=1, act_fu='relu', size_denseLayer=500,\n",
    "                 data_augmentation=False, batch_normalisation=True, input_channels=3,\n",
    "                 classes=10, padding=1, kernel_org=1, dropout_rate=0.2, num_filters=4):\n",
    "        \n",
    "        # call the parent constructor\n",
    "        super(ConvNeuNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm = batch_normalisation\n",
    "        self.data_aug = data_augmentation\n",
    "        self.width = 150\n",
    "        self.height = 150\n",
    "        \n",
    "        #(batch_size = 64, input_channels=3, width=150, height=150)\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=input_channels, out_channels=num_filters,\n",
    "                    kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        self.width = ((self.width- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=num_filters)\n",
    "        self.af1 = ReLU() if act_fu=='relu' else gelu() if act_fu=='gelu' else selu() if act_fu=='selu' else mish()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        self.width = ((self.width- size_kernel)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel)/num_stride) + 1\n",
    "        \n",
    "        # Add dropout layer\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        size_kernel = size_kernel*kernel_org\n",
    "        self.conv2 = Conv2d(in_channels=num_filters, out_channels=num_filters,\n",
    "                     kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        self.width = ((self.width- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=num_filters)\n",
    "        self.af2 = ReLU() if act_fu=='relu' else gelu() if act_fu=='gelu' else selu() if act_fu=='selu' else mish()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        self.width = ((self.width- size_kernel)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel)/num_stride) + 1\n",
    "        # initialize third set of CONV => RELU => POOL layers\n",
    "        size_kernel = size_kernel*kernel_org\n",
    "        self.conv3 = Conv2d(in_channels=num_filters, out_channels=num_filters,\n",
    "                     kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        self.width = ((self.width- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=num_filters)\n",
    "        self.af3 = ReLU() if act_fu=='relu' else gelu() if act_fu=='gelu' else selu() if act_fu=='selu' else mish()\n",
    "        self.maxpool3 = MaxPool2d(kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        \n",
    "         # updating width and height of the next layer after maxpool\n",
    "        self.width = ((self.width- size_kernel)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel)/num_stride) + 1\n",
    "        # initialize fourth set of CONV => RELU => POOL layers\n",
    "        size_kernel = size_kernel*kernel_org\n",
    "        self.conv4 = Conv2d(in_channels=num_filters, out_channels=num_filters,\n",
    "                     kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        self.width = ((self.width- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=num_filters)\n",
    "        self.af4 = ReLU() if act_fu=='relu' else gelu() if act_fu=='gelu' else selu() if act_fu=='selu' else mish()\n",
    "        self.maxpool4 = MaxPool2d(kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        self.width = ((self.width- size_kernel)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel)/num_stride) + 1\n",
    "        # initialize fifth set of CONV => RELU => POOL layers\n",
    "        size_kernel = size_kernel*kernel_org\n",
    "        self.conv5 = Conv2d(in_channels=num_filters, out_channels=num_filters,\n",
    "                     kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        self.width = ((self.width- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel + 2*padding)/num_stride) + 1\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=num_filters)\n",
    "        self.af5 = ReLU() if act_fu=='relu' else gelu() if act_fu=='gelu' else selu() if act_fu=='selu' else mish()\n",
    "        self.maxpool5 = MaxPool2d(kernel_size=size_kernel, stride=num_stride, padding=padding)\n",
    "        \n",
    "        # updating width and height of the next layer after maxpool\n",
    "        self.width = ((self.width- size_kernel)/num_stride) + 1\n",
    "        self.height = ((self.height- size_kernel)/num_stride) + 1\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=int(num_filters*self.width*self.height), out_features=size_denseLayer)\n",
    "        self.af6 = ReLU() if act_fu=='relu' else gelu() if act_fu=='gelu' else selu() if act_fu=='selu' else mish()\n",
    "        \n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=size_denseLayer, out_features=classes)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pass the input through our first set of CONV => Batch_norm => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.af1(x)\n",
    "        x = self.maxpool1(x)\n",
    "       \n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => Batch_norm => RELU => layers\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.af2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # pass the output from the previous layer through the third\n",
    "        # set of CONV => Batch_norm => RELU => POOL layers\n",
    "        x = self.conv3(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn3(x)\n",
    "        x = self.af3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        print(x.shape)\n",
    "        # pass the output from the previous layer through the fourth\n",
    "        # set of CONV => Batch_norm => RELU => POOL layers\n",
    "        x = self.conv4(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn4(x)\n",
    "        x = self.af4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        # pass the output from the previous layer through the fifth\n",
    "        # set of CONV => Batch_norm => RELU => POOL layers\n",
    "        x = self.conv5(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn5(x)\n",
    "        x = self.af5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.af6(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa62aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee960545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer and loss function\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b675563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f604f741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4, 150, 150])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x90000 and 78400x500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HICLIP~1\\AppData\\Local\\Temp/ipykernel_20976/1185727478.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HICLIP~1\\AppData\\Local\\Temp/ipykernel_20976/2825666439.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m# through our only set of FC => RELU layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maf6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# pass the output to our softmax classifier to get our output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x90000 and 78400x500)"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluating and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        #print(images.shape())\n",
    "        if torch.cuda.is_available():\n",
    "            images=images.cuda()\n",
    "            labels=labels.cuda()\n",
    "        \n",
    "        # make grad to zero after each batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/len(train_dataset)\n",
    "    train_loss=train_loss/len(train_dataset)\n",
    "        \n",
    "    \n",
    "    #Evaluating on validation dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=images.cuda()\n",
    "            labels=labels.cuda()\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    test_accuracy=test_accuracy/len(test_dataset)\n",
    "    \n",
    "    print(\"Epoch: \"+str(epoch)+ 'Train Loss: '+str(int(train_loss))+'Train Accuracy: '+\n",
    "          str(int(train_accuracy))+ 'Test Accuracy: '+str(int(test_accuracy)))  \n",
    "    \n",
    "    \n",
    "    # save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49d2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614017e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
